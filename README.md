# 背景 #
现代数据库存储引擎所面临的读写场景大致可以分为两类：读多写少和写多读少，读多写少的场景主要采用基于B+树的数据库，例如MySQL，而写多读少的场景则主要是采用本项目的主角：LSM-Tree，代表的产品是RocksDB

# 需求分析 #
对于写多读少的场景，影响系统整体效率的最关键因素就是写操作时延，传统的磁盘写方式是随机写，就是根据KV对信息找到数据存放在磁盘的位置然后进行数据更新

这种方式在每次写操作时都需要磁头进行寻道，当写请求数量变得无比庞大时，写请求的响应时延就很低

解决这个问题的方法很简单，就是采用追加写的方式，每一次写入数据都会在文件的末尾追加一个新的KV对，这就避免了磁头的寻道，提高了写操作的效率

但这样做也会带来一些坏处，主要有两点：数据冗余和读性能低

追加写会导致同一组KV对存储多个冗余记录，除了最后一个此前所有记录都是无用的，在极端情况下，只需一个KV对记录，进行无限次的写操作就会占满整个磁盘空间

另一个问题就是读性能降低，因为每一次查询都需要从后向前回溯，直到找到最新的KV数据记录，这样操作的最坏时间复杂度是O(N)，显然无法满足需求

#  方案设计 #

## 压缩合并————解决数据冗余 ##
压缩合并方案的提出是为了解决数据冗余的问题，他的思想很简单，就是将没有用的旧数据给删除掉，这个操作成为compact，但真正实施起来还需要解决一些细节问题

首先就是资源争用，compact操作和write操作是并发执行的，这就需要对数据加锁来管理资源争用，这样子就会大大降低系统效率。解决方法就是将文件分块，称为Table，每一个Table都有最大容量，例如4KB，当追加写的数据超过这个容量之后就会触发Table分裂，生成一个新Table。这样一来，compact操作就只用在旧的Table中执行，而write操作就在最新的Table中追加新数据就行了。通过这样解耦就可以降低资源争用的开销

## 数据有序存储————提升读性能 ##
数据有序存储是为了解决读性能低的问题，基于上一节讲到的分块，我们对每一个Table内的数据进行有序存储，这样在进行数据查找时就可以使用二分查找的方法来降低读时间复杂度

Table内有序的方案设计违背的追加写无脑往后加数据的设定，所以需要引入额外的方案来实现这个方案，具体使用的就是将Table分成了MemTable和SSTable，前者存放在主存中，后者存放在磁盘中。
用户的写操作唯一入口是MemTable，也就是说所有写入的数据都会首先经过MemTable，而数据进入MemTable之后采用就地写操作来保证数据的有序性，当数据量达到一定程度之后就会溢写到磁盘中(flush)。
这样做实际上是以Table为粒度进行追加写，而Table内的数据天然是有序的，而SSTable是由MemTable写入的，所以也是有序的。

这样的设计还会带来三个问题：内存崩溃了怎么办？flush和用户的write产生资源冲突怎么办？MemTable内部采用什么样的数据结构实现？

## MemTable应该解决的问题 ##
![lsm-tree](https://github.com/GaomingY/MiniDB/blob/main/image/LSM-Tree.png)
### WAL ###
使用WAL(Write-ahead log)预写日志技术：用户的写请求首先会生成一个write-ahead log保存在磁盘里，之后再将数据写入到内存的MemTable中，这样即使内存崩溃了，也可以根据日志内容重建数据
WAL采用追加写方式，属于磁盘顺序IO，不会产生性能瓶颈，而且每当一个MemTable溢写到磁盘中后，它对应的日志记录就会删除，WAL也不会带来很高的存储开销

### 只读分层 ###
将MemTable进一步划分成activate MemTable和readonly MemTable，用户的写请求的数据都只会写入activate MemTable中，当MemTable容量满了之后就会变成readonly MemTable，之后将不会再有新数据写入，
readonly MemTable执行flush操作
这样一来，用户的write操作在activate MemTable中执行，存储引擎的flush操作在readonly MemTable中执行，就完美解决了资源争用的问题

### SkipList ###
讲了这么多MemTable的设计细节，那么应该用什么样的数据结构来实现它呢？有两个候选者，红黑树和跳表(SkipList)，二者的性能相当，但跳表的实现更为简单，加锁操作也更容易，所以基于LSM-Tree的存储引擎基本上都使用跳表作为MemTable的实现方法

## SSTable应该解决的问题 ##
MemTable解决的问题实际上都是属于第二点，数据有序存储方面。而第一点，压缩合并，就是由SSTable来完成
实际上，用户写的数据只会写入activate MemTable中，因此，在磁盘中的SSTable都是old Table，都是可以进行压缩合并的

每一个SSTable天然具备两个优势：SSTable内部不存在重复KV对数据，SSTable内部的KV对数据是有序的。但不同Table之间的数据可能有重复，而且不是有序的
因此，在实现SSTable的compact操作时必须要解决这两个问题
SStable具备以下特点：每向下一层的SSTable的容量都会增大一定倍数；level0比较特殊，这层只有局部有序和唯一，也就是单个table内，但level1到levelk中单层之内都没有冗余数据且整层全局有序

由于SSTable到了深层之后容量就会很大，造成读操作非常笨重，因此，SSTable进一步做了一些优化

### 拆分成block ###
SSTable内部进一步把table拆分成多个block，记录了每个block的k_min和k_max以及块中每行的k_min和k_max，其次LSM-Tree还记录了不同level中的SSTable中的k_min和k_max
这样记录有助于不同table之间的归并排序

### bloomfilter ###
使用bloomfilter可以在O(1)时间内快速判断一个K是否存在于当前table中，但bloomfilter有假阳性的缺陷，所以判断出来不存在的k一定不存在，但判断出来存在的k却有可能不存在，因此需要引入更多的机制进行double check

# 读写流程 #
最后，用两张图来展示LSM-Tree的读写流程
![read progress](https://github.com/GaomingY/MiniDB/blob/main/image/read.png)
![write progress](https://github.com/GaomingY/MiniDB/blob/main/image/write.png)

# 优化方法 #
为了能更加充分地利用存储空间，MiniDB采取了一系列数据压缩的方法来减少存储资源占用，包括变长整数压缩和前缀压缩

## 变长整数压缩 ##
因为这个压缩方法主要是针对正整数进行压缩的，所以应用到MiniDB中之后就是对table中存储的Key和Value的长度进行压缩
定长整数一般是4字节或8字节，从低位开始，数七位数据组成一个字节，这个字节最高位是标志位，表示之后还有没有字节了，是1就是有，是0就没有，然后将这七位数据和标志位合并成一个字节
最后得到的二进制数就是压缩过后的数据，值得注意的是，如果值比较大，本来就占满4字节了，那么压缩过后就会变成5字节，占用空间反而变大了！
但这种情况发生的概率并不高，因为压缩亏损的临界点是2^28，也就是说小于这个值的压缩都很赚，大于这个值的才会亏损，这是一个很大的值，在key和value大小这个场景中是很难出现亏损的

## 前缀压缩 ##







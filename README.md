 # 背景 #
现代数据库存储引擎所面临的读写场景大致可以分为两类：读多写少和写多读少，读多写少的场景主要采用基于B+树的数据库，例如MySQL，而写多读少的场景则主要是采用本项目的主角：LSM-Tree，代表的产品是RocksDB

# 需求分析 #
对于写多读少的场景，影响系统整体效率的最关键因素就是写操作时延，传统的磁盘写方式是随机写，就是根据KV对信息找到数据存放在磁盘的位置然后进行数据更新

这种方式在每次写操作时都需要磁头进行寻道，当写请求数量变得无比庞大时，写请求的响应时延就很低

解决这个问题的方法很简单，就是采用追加写的方式，每一次写入数据都会在文件的末尾追加一个新的KV对，这就避免了磁头的寻道，提高了写操作的效率

但这样做也会带来一些坏处，主要有两点：数据冗余和读性能低

追加写会导致同一组KV对存储多个冗余记录，除了最后一个此前所有记录都是无用的，在极端情况下，只需一个KV对记录，进行无限次的写操作就会占满整个磁盘空间

另一个问题就是读性能降低，因为每一次查询都需要从后向前回溯，直到找到最新的KV数据记录，这样操作的最坏时间复杂度是O(N)，显然无法满足需求

#  方案设计 #

## 压缩合并 ##
压缩合并方案的提出是为了解决数据冗余的问题，他的思想很简单，就是将没有用的旧数据给删除掉，这个操作成为compact，但真正实施起来还需要解决一些细节问题

首先就是资源争用，compact操作和write操作是并发执行的，这就需要对数据加锁来管理资源争用，这样子就会大大降低系统效率。解决方法就是将文件分块，称为Table，每一个Table都有最大容量，例如4KB，当追加写的数据超过这个容量之后就会触发Table分裂，生成一个新Table。这样一来，compact操作就只用在旧的Table中执行，而write操作就在最新的Table中追加新数据就行了。通过这样解耦就可以降低资源争用的开销

## 数据有序存储 ##
数据有序存储是为了解决读性能低的问题，基于上一节讲到的分块，我们对每一个Table内的数据进行有序存储，这样在进行数据查找时就可以使用二分查找的方法来降低读时间复杂度

Table内有序的方案设计违背的追加写无脑往后加数据的设定，所以需要引入额外的方案来实现这个方案，具体使用的就是将Table分成了MemTable和SSTable，前者存放在主存中，后者存放在磁盘中。
用户的写操作唯一入口是MemTable，也就是说所有写入的数据都会首先经过MemTable，而数据进入MemTable之后采用就地写操作来保证数据的有序性，当数据量达到一定程度之后就会溢写到磁盘中(flush)。
这样做实际上是以Table为粒度进行追加写，而Table内的数据天然是有序的，而SSTable是由MemTable写入的，所以也是有序的。

这样的设计还会带来三个问题：内存崩溃了怎么办？flush和用户的write产生资源冲突怎么办？MemTable内部采用什么样的数据结构实现？

## MemTable应该解决的问题 ##
### WAL ###
使用WAL(Write-ahead log)预写日志技术：用户的写请求首先会生成一个write-ahead log保存在磁盘里，之后再将数据写入到内存的MemTable中，这样即使内存崩溃了，也可以根据日志内容重建数据
WAL采用追加写方式，属于磁盘顺序IO，不会产生性能瓶颈，而且每当一个MemTable溢写到磁盘中后，它对应的日志记录就会删除，WAL也不会带来很高的存储开销

### 只读分层 ###
将MemTable进一步划分成activate MemTable和readonly MemTable，用户的写请求的数据都只会写入activate MemTable中，当MemTable容量满了之后就会变成readonly MemTable，之后将不会再有新数据写入，
readonly MemTable执行flush操作
这样一来，用户的write操作在activate MemTable中执行，存储引擎的flush操作在readonly MemTable中执行，就完美解决了资源争用的问题

### SkipList ###
讲了这么多MemTable的设计细节，那么应该用什么样的数据结构来实现它呢？有两个候选者，红黑树和跳表(SkipList)，二者的性能相当，但跳表的实现更为简单，加锁操作也更容易，所以基于LSM-Tree的存储引擎基本上都使用跳表作为MemTable的实现方法

## SSTable应该解决的问题 ##

